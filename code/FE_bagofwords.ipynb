{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\macie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\macie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from collections import Counter\n",
    "# import string\n",
    "import nltk\n",
    "# import warnings\n",
    "# %matplotlib inline\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"..//data//x_train.csv\", encoding=\"latin-1\")\n",
    "y_train = pd.read_csv(\"..//data//y_train.csv\", encoding=\"latin-1\")\n",
    "x_test = pd.read_csv(\"..//data//x_test.csv\", encoding=\"latin-1\")\n",
    "y_test = pd.read_csv(\"..//data//y_test.csv\", encoding=\"latin-1\")\n",
    "x_valid = pd.read_csv(\"..//data//x_valid.csv\", encoding=\"latin-1\")\n",
    "y_valid = pd.read_csv(\"..//data//y_valid.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513801, 5), (513801, 1), (220201, 5), (220201, 1))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>combined_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810677</th>\n",
       "      <td>working listening to kmps happy my boss amp th...</td>\n",
       "      <td>work listen to kmps happy my bos amp the mecha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684982</th>\n",
       "      <td>we both know she is late for quotat 11quot</td>\n",
       "      <td>we both know she be late for quotat 11quot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954730</th>\n",
       "      <td>do not bogart that joint my friend</td>\n",
       "      <td>do not bogart that joint my friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142760</th>\n",
       "      <td>i am so sick of being sick i do not want to mi...</td>\n",
       "      <td>i be so sick of be sick i do not want to miss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200331</th>\n",
       "      <td>i am pretty sure i went to bed about 2 hours a...</td>\n",
       "      <td>i be pretty sure i go to bed about 2 hour ago ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023596</th>\n",
       "      <td>good morning sunshine</td>\n",
       "      <td>good morning sunshine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281932</th>\n",
       "      <td>is the website  down  i cannot get it to load</td>\n",
       "      <td>be the website down i cannot get it to load</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828817</th>\n",
       "      <td>i listened to the itunes samplesmy fav albums...</td>\n",
       "      <td>i listen to the itunes samplesmy fav album be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683555</th>\n",
       "      <td>i use palringo for chat and fring for voice c...</td>\n",
       "      <td>i use palringo for chat and fring for voice ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603860</th>\n",
       "      <td>i am uploading photos of my friendsbut it does...</td>\n",
       "      <td>i be upload photo of my friendsbut it do not work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513801 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Text  \\\n",
       "810677   working listening to kmps happy my boss amp th...   \n",
       "684982         we both know she is late for quotat 11quot    \n",
       "954730               do not bogart that joint my friend      \n",
       "142760   i am so sick of being sick i do not want to mi...   \n",
       "200331   i am pretty sure i went to bed about 2 hours a...   \n",
       "...                                                    ...   \n",
       "1023596                            good morning sunshine     \n",
       "281932     is the website  down  i cannot get it to load     \n",
       "828817    i listened to the itunes samplesmy fav albums...   \n",
       "683555    i use palringo for chat and fring for voice c...   \n",
       "603860   i am uploading photos of my friendsbut it does...   \n",
       "\n",
       "                                            combined_tweet  \n",
       "810677   work listen to kmps happy my bos amp the mecha...  \n",
       "684982          we both know she be late for quotat 11quot  \n",
       "954730                  do not bogart that joint my friend  \n",
       "142760   i be so sick of be sick i do not want to miss ...  \n",
       "200331   i be pretty sure i go to bed about 2 hour ago ...  \n",
       "...                                                    ...  \n",
       "1023596                              good morning sunshine  \n",
       "281932         be the website down i cannot get it to load  \n",
       "828817   i listen to the itunes samplesmy fav album be ...  \n",
       "683555   i use palringo for chat and fring for voice ca...  \n",
       "603860   i be upload photo of my friendsbut it do not work  \n",
       "\n",
       "[513801 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.drop(['ID', 'Date', 'flag', 'User'], axis = 'columns')\n",
    "# removing unnecessary user tags\n",
    "x_train['Text'] = x_train['Text'].replace(r\"@\\w+\", \"\", regex=True)\n",
    "# resolving contractions (and slang)\n",
    "x_train['Text'] = x_train['Text'].apply(lambda x: contractions.fix(x))\n",
    "# removing punctuation marks\n",
    "x_train['Text'] = x_train['Text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "# deleting websites\n",
    "x_train['Text'] = x_train['Text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "# lowercasing letters in the text\n",
    "x_train['Text'] = x_train['Text'].str.lower()\n",
    "#x_train['Text'] = x_train['Text'].apply(lambda x: \" \".join([w for w in x.split() if len(w) >= 2]))\n",
    "# individual words considered as tokens\n",
    "tokenized_tweet = x_train['Text'].apply(lambda x: x.split())\n",
    "# Initialize wordnet lemmatizer only on verbs - makes the biggest sense\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='v') for word in s])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='n') for word in s])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='a') for word in s])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='r') for word in s])\n",
    "# combining to sentences\n",
    "combined_sentences = [' '.join(tokens) for tokens in tokenized_tweet]\n",
    "x_train['combined_tweet'] = combined_sentences\n",
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bill',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'call',\n",
       " 'co',\n",
       " 'con',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'do',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'in',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'interest',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mill',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'none',\n",
       " 'nor',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'rather',\n",
       " 're',\n",
       " 'same',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'sincere',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'system',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_words = CountVectorizer(stop_words='english').get_stop_words()\n",
    "custom_stop_words = set(custom_stop_words) - {'not','alone','why','well','very','together','such','nobody','noone','nothing','myself','cry','cannot','cant','can','could','couldnt'}\n",
    "custom_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df = 0.95, min_df = 5, max_features = 5000, stop_words=custom_stop_words)\n",
    "bow = bow_vectorizer.fit_transform(x_train['combined_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>combined_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240689</th>\n",
       "      <td>tierd and it is school tomorrow  last week atl...</td>\n",
       "      <td>tierd and it be school tomorrow last week atleast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413003</th>\n",
       "      <td>twitter gets boring n boring everydayno star w...</td>\n",
       "      <td>twitter get bore n bore everydayno star want 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950284</th>\n",
       "      <td>i am watching guy ripley right nowhahahilarious</td>\n",
       "      <td>i be watch guy ripley right nowhahahilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672298</th>\n",
       "      <td>that is the way indoor stadium toilets are</td>\n",
       "      <td>that be the way indoor stadium toilet be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852721</th>\n",
       "      <td>it must be all that bike riding</td>\n",
       "      <td>it must be all that bike rid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55759</th>\n",
       "      <td>wantd 2b comedian when lil boy i memrize comm...</td>\n",
       "      <td>wantd 2b comedian when lil boy i memrize comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175608</th>\n",
       "      <td>omg i cannot believe jay leno is going off the...</td>\n",
       "      <td>omg i cannot believe jay leno be go off the air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661283</th>\n",
       "      <td>i do not know my days are all messed up since...</td>\n",
       "      <td>i do not know my day be all mess up since i ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43369</th>\n",
       "      <td>so i am guessin  meant midnight pacific time</td>\n",
       "      <td>so i be guessin mean midnight pacific time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401275</th>\n",
       "      <td>shit fuckin fever fuckin body think i am going...</td>\n",
       "      <td>shit fuckin fever fuckin body think i be go to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220201 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  \\\n",
       "240689  tierd and it is school tomorrow  last week atl...   \n",
       "413003  twitter gets boring n boring everydayno star w...   \n",
       "950284   i am watching guy ripley right nowhahahilarious    \n",
       "672298        that is the way indoor stadium toilets are    \n",
       "852721                   it must be all that bike riding    \n",
       "...                                                   ...   \n",
       "55759    wantd 2b comedian when lil boy i memrize comm...   \n",
       "175608  omg i cannot believe jay leno is going off the...   \n",
       "661283   i do not know my days are all messed up since...   \n",
       "43369       so i am guessin  meant midnight pacific time    \n",
       "401275  shit fuckin fever fuckin body think i am going...   \n",
       "\n",
       "                                           combined_tweet  \n",
       "240689  tierd and it be school tomorrow last week atleast  \n",
       "413003  twitter get bore n bore everydayno star want 2...  \n",
       "950284       i be watch guy ripley right nowhahahilarious  \n",
       "672298           that be the way indoor stadium toilet be  \n",
       "852721                       it must be all that bike rid  \n",
       "...                                                   ...  \n",
       "55759   wantd 2b comedian when lil boy i memrize comme...  \n",
       "175608    omg i cannot believe jay leno be go off the air  \n",
       "661283  i do not know my day be all mess up since i ge...  \n",
       "43369          so i be guessin mean midnight pacific time  \n",
       "401275  shit fuckin fever fuckin body think i be go to...  \n",
       "\n",
       "[220201 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid = x_valid.drop(['ID', 'Date', 'flag', 'User'], axis = 'columns')\n",
    "# removing unnecessary user tags\n",
    "x_valid['Text'] = x_valid['Text'].replace(r\"@\\w+\", \"\", regex=True)\n",
    "# resolving contractions (and slang)\n",
    "x_valid['Text'] = x_valid['Text'].apply(lambda x: contractions.fix(x))\n",
    "# removing punctuation marks\n",
    "x_valid['Text'] = x_valid['Text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "# deleting websites\n",
    "x_valid['Text'] = x_valid['Text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "# lowercasing letters in the text\n",
    "x_valid['Text'] = x_valid['Text'].str.lower()\n",
    "#x_valid['Text'] = x_valid['Text'].apply(lambda x: \" \".join([w for w in x.split() if len(w) >= 2]))\n",
    "# individual words considered as tokens\n",
    "tokenized_tweet = x_valid['Text'].apply(lambda x: x.split())\n",
    "# Initialize wordnet lemmatizer only on verbs - makes the biggest sense\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='v') for word in s])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='n') for word in s])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='a') for word in s])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda s: [wnl.lemmatize(word, pos='r') for word in s])\n",
    "# combining to sentences\n",
    "combined_sentences = [' '.join(tokens) for tokens in tokenized_tweet]\n",
    "x_valid['combined_tweet'] = combined_sentences\n",
    "x_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_vectorizer = CountVectorizer(max_df=0.99, min_df=10, max_features=1000, stop_words=custom_stop_words)\n",
    "bow = bow_vectorizer.transform(x_valid['combined_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132324"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "pred = model.predict(bow)\n",
    "pred\n",
    "sum(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535216219383569"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_valid, pred, pos_label=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8269853452073334"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
